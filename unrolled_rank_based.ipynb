{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_horses_orig(path, image_size, data_shape):\n",
    "    mask_path = path + 'masks/'\n",
    "    image_path = path + 'images/'\n",
    "    images = []\n",
    "    masks = []\n",
    "    test_images = []\n",
    "    test_masks = []\n",
    "    \n",
    "    for i in range(328):\n",
    "        \n",
    "        orig_im = cv2.imread(image_path + 'image-{}.png'.format(i))\n",
    "        orig_im= cv2.cvtColor(orig_im, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        low_im = cv2.resize(orig_im, dsize=(image_size, image_size))\n",
    "\n",
    "        orig_mask = cv2.imread(mask_path + 'mask-{}.png'.format(i))\n",
    "        low_mask = cv2.resize(orig_mask, dsize=(image_size, image_size))\n",
    "        low_mask = cv2.cvtColor(low_mask, cv2.COLOR_RGB2GRAY)\n",
    "        bin_mask = (low_mask > 0) + 0\n",
    "        \n",
    "        images.append(low_im)\n",
    "        masks.append(bin_mask)\n",
    "    \n",
    "    if data_shape == '3d':\n",
    "        xtest = np.reshape(np.array(images[250:]), (-1,image_size, image_size, 3))\n",
    "        ytest = np.reshape(np.array(masks[250:]), (-1, image_size, image_size, 1))\n",
    "        xdata = np.reshape(np.array(images[:200]), (-1,image_size, image_size, 3))\n",
    "        ydata = np.reshape(np.array(masks[:200]), (-1, image_size, image_size, 1))\n",
    "        yval =  np.reshape(np.array(masks[200:250]), (-1, image_size, image_size, 1))\n",
    "        xval = np.reshape(np.array(images[200:250]), (-1,image_size, image_size, 3))\n",
    "    else:\n",
    "        xtest = np.reshape(np.array(images[250:]), (-1, image_size * image_size * 3))\n",
    "        ytest = np.reshape(np.array(masks[250:]), (-1, image_size * image_size))\n",
    "        xdata = np.reshape(np.array(images[:200]), (-1, image_size * image_size * 3))\n",
    "        ydata = np.reshape(np.array(masks[:200]), (-1, image_size * image_size))\n",
    "        yval =  np.reshape(np.array(masks[200:250]), (-1, image_size * image_size))\n",
    "        xval = np.reshape(np.array(images[200:250]), (-1, image_size * image_size * 3))\n",
    "    return xdata, xval, xtest, ydata, yval, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the path address \n",
    "path = './horses/'\n",
    "image_size = 32\n",
    "data_shape = '2d'\n",
    "xdata, xval, xtest, ydata, yval, ytest = load_horses_orig(path, image_size, data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_shape == '3d':\n",
    "    def draw(image, real_mask, fake_mask):\n",
    "        fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        ax3.axis('off')\n",
    "        ax1.imshow(image)\n",
    "        ax2.imshow(real_mask, cmap=plt.cm.gray)\n",
    "        ax3.imshow(fake_mask, cmap=plt.cm.gray)\n",
    "        plt.show()\n",
    "else:\n",
    "    def draw(image, mask, fake):\n",
    "        fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        ax3.axis('off')\n",
    "        ax1.imshow(np.reshape(image, (image_size,image_size,3)))\n",
    "        ax2.imshow(np.reshape(mask, (image_size,image_size,1)), cmap=plt.cm.gray)\n",
    "        ax3.imshow(np.reshape(fake, (image_size,image_size,1)), cmap=plt.cm.gray)\n",
    "        plt.show()\n",
    "        \n",
    "draw(xdata[0], ydata[0], ydata[0])\n",
    "print(xdata[0].shape, ydata[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "def iou(ytrue, yprediction, data_shape):\n",
    "    yp = yprediction\n",
    "    yt = ytrue\n",
    "    yp = yp > 0.5 + 0\n",
    "    if data_shape == '3d':\n",
    "        intersect = np.sum(np.minimum(yp, yt))\n",
    "        union = np.sum(np.maximum(yp, yt))\n",
    "    else:\n",
    "        intersect = np.sum(np.minimum(yp, yt),1)\n",
    "        union = np.sum(np.maximum(yp, yt),1)\n",
    "    \n",
    "    return np.average(intersect / (union+0.0))\n",
    "\n",
    "assert iou(ydata, ydata, data_shape) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.concat_layer = tf.keras.layers.Concatenate(axis=3)\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(1, 3, 2, activation = tf.nn.softplus)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 3, 2, activation = tf.nn.softplus)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, 3, 2, activation = tf.nn.softplus)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(128, 3, 2, activation = tf.nn.softplus)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc_score = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, yt, y):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        yt = tf.cast(yt, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "\n",
    "        x = self.concat_layer([x, yt, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_score(x)\n",
    "        return x\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.concat_layer = tf.keras.layers.Concatenate(axis=3)\n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(128, 3, 2, activation = tf.nn.softplus, padding = 'same')\n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(64, 3, 2, activation = tf.nn.softplus, padding = 'same')\n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(32, 3, 2, activation = tf.nn.softplus, padding = 'same')\n",
    "        self.conv4 = tf.keras.layers.Conv2DTranspose(1, 3, 2, activation = tf.nn.softplus, padding = 'same')\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc_energy = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, z):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        x = self.concat_layer([x, z])\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_energy(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rank_based_training(self):\n",
    "#     # what are value_h and l ? how are they calculated..\n",
    "    \n",
    "#     self.margin_weight_ph = tf.placeholder(tf.float32, shape=[], name=\"Margin\")\n",
    "#     self.value_h = tf.placeholder(tf.float32, shape=[None])\n",
    "#     self.value_l = tf.placeholder(tf.float32, shape=[None])\n",
    "#     self.yp_h_ind = tf.placeholder(tf.float32,\n",
    "#                           shape=[None, self.config.output_num * self.config.dimension],\n",
    "#                           name=\"YP_H\")\n",
    "\n",
    "#     self.yp_l_ind = tf.placeholder(tf.float32,\n",
    "#                           shape=[None, self.config.output_num * self.config.dimension],\n",
    "#                           name=\"YP_L\")\n",
    "\n",
    "#     self.energy_yh = self.get_energy(xinput=self.x, yinput=self.yp_h_ind, embedding=self.embedding,\n",
    "#                                      reuse=self.config.pretrain)\n",
    "#     self.energy_yl = self.get_energy(xinput=self.x, yinput=self.yp_l_ind, embedding=self.embedding,\n",
    "#                                      reuse=True)\n",
    "\n",
    "\n",
    "#     self.energy_yp = self.energy_yh\n",
    "#     self.yp = self.yp_h_ind\n",
    "\n",
    "#     self.energy_ygradient = tf.gradients(self.energy_yp, self.yp)[0]\n",
    "\n",
    "#     vloss = 0\n",
    "#     for v in self.spen_variables():\n",
    "#         vloss = vloss + tf.nn.l2_loss(v)\n",
    "\n",
    "#     obj1 = tf.reduce_sum( tf.maximum( (self.value_h - self.value_l)*self.margin_weight_ph - self.energy_yh + self.energy_yl, 0.0))\n",
    "#     self.vh_sum = tf.reduce_sum (self.value_h)\n",
    "#     self.vl_sum = tf.reduce_sum (self.value_l)\n",
    "#     self.eh_sum = tf.reduce_sum(self.energy_yh)\n",
    "#     self.el_sum = tf.reduce_sum(self.energy_yl)\n",
    "#     self.objective = obj1 +  self.config.l2_penalty * vloss #+ obj2\n",
    "#     self.num_update = tf.reduce_sum(tf.cast( (self.value_h - self.value_l)*self.margin_weight_ph  >= (self.energy_yh - self.energy_yl), tf.float32))\n",
    "#     self.train_step = self.optimizer.minimize(self.objective, var_list=self.spen_variables())\n",
    "#     return self\n",
    "\n",
    "# def search_better_y_fast(self, xtest, yprev):\n",
    "#     final_best = np.zeros((xtest.shape[0], self.config.output_num))\n",
    "#     for iter in range(np.shape(xtest)[0]):\n",
    "#       random_proposal = yprev[iter,:]\n",
    "#       score_first = self.evaluate(np.expand_dims(xtest[iter], 0), np.expand_dims(random_proposal, 0))\n",
    "#       start = score_first\n",
    "#       labelset = set(np.arange(self.config.dimension))\n",
    "#       found = False\n",
    "#       for l in range(self.config.output_num):\n",
    "#         for label in (labelset - set([yprev[iter,l]])): #set([random_proposal[l]]):\n",
    "#           random_proposal_new = random_proposal[:]\n",
    "#           random_proposal_new[l] = label\n",
    "#           score = self.evaluate(np.expand_dims(xtest[iter], 0),\n",
    "#                                 np.expand_dims(random_proposal_new, 0))\n",
    "#           if score > score_first:\n",
    "#             score_first = score\n",
    "#             best_l = l\n",
    "#             best_label = label\n",
    "#             found = True\n",
    "#             #random_proposal[l] = random_proposal_new[l]\n",
    "#             #changed = True\n",
    "#             #break\n",
    "#       if self.config.loglevel > 4:\n",
    "#         print (\"iter:\", iter, \"found:\", found, \"score first: \", start, \"new score\", score_first)\n",
    "#       final_best[iter, :] = yprev[iter, :]\n",
    "#       if found:\n",
    "#         final_best[iter, best_l] = best_label\n",
    "#     return final_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use y_true only at reward not at gradient computation\n",
    "def reward(yprediction, ytrue):\n",
    "    intersect = tf.math.reduce_sum(tf.math.minimum(yprediction, ytrue))\n",
    "    union = tf.math.reduce_sum(tf.math.maximum(yprediction, ytrue))\n",
    "    return tf.math.reduce_mean(tf.math.divide(intersect, union))\n",
    "\n",
    "def search_better_y_fast(xtest, yprev):\n",
    "    # yprev = sample from langevin inf?\n",
    "    final_best = np.zeros((xtest.shape[0], image_size * image_size))\n",
    "    for i in range(np.shape(xtest)[0]):\n",
    "        random_proposal = yprev[i,:]\n",
    "        score_first = reward(random_proposal, xtest[i])\n",
    "        start = score_first\n",
    "        labelset = set(np.arange(1)) #output dim = 1?\n",
    "        found = False\n",
    "        for l in range(image_size * image_size):\n",
    "            for label in (labelset - set([yprev[i,l]])):\n",
    "                random_proposal_new = random_proposal[:]\n",
    "                random_proposal_new[l] = label\n",
    "                score = reward(random_proposal_new, xtest[i])\n",
    "                \n",
    "            if score > score_first:\n",
    "                score_first = score\n",
    "                best_l = l\n",
    "                best_label = label\n",
    "                found = True\n",
    "        \n",
    "        print (\"iter:\", i, \"found:\", found, \"score first: \", start, \"new score\", score_first)\n",
    "        final_best[i, :] = yprev[i, :]\n",
    "        if found:\n",
    "            final_best[i, best_l] = best_label\n",
    "    return final_best\n",
    "\n",
    "def rank_based_training(real_image, inf_iter, inf_rate, l2_penalty):\n",
    "    batch_size = real_image.get_shape().as_list()[0]\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    energy_yh = gen(real_image, z)\n",
    "    fake_label = unrolled_inf(real_image, z, inf_iter, inf_rate)\n",
    "    \n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    energy_yl = gen(real_image, z)\n",
    "\n",
    "    energy_yp = energy_yh\n",
    "    \n",
    "    # sort the values based on energy\n",
    "    # margin > 0 always\n",
    "    # y_h = y_n in paper (change sign)\n",
    "    # \n",
    "\n",
    "    gradients = compute_gradient(real_image, fake_label)\n",
    "\n",
    "    vloss = 0\n",
    "    for v in gen.trainable_variables:\n",
    "        vloss = vloss + tf.nn.l2_loss(v)\n",
    "\n",
    "    obj1 = tf.reduce_sum( tf.maximum( (value_h - value_l) * margin_weight_ph - energy_yh + energy_yl, 0.0))\n",
    "    vh_sum = tf.reduce_sum (value_h)\n",
    "    vl_sum = tf.reduce_sum (value_l)\n",
    "    eh_sum = tf.reduce_sum(energy_yh)\n",
    "    el_sum = tf.reduce_sum(energy_yl)\n",
    "    objective = obj1 +  l2_penalty * vloss #+ obj2\n",
    "    num_update = tf.reduce_sum(tf.cast( (value_h - value_l)*margin_weight_ph  >= (energy_yh - energy_yl), tf.float32))\n",
    "    train_step = optimizer.minimize(objective, var_list=gen.trainable_variables)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y):\n",
    "    y = tf.Variable(y)\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(y)\n",
    "        energy = gen(x, y)\n",
    "    return t.gradient(energy, y)\n",
    "\n",
    "def unrolled_inf(images, fake_label, inf_iter, inf_rate):\n",
    "    yp_ind = fake_label; current_yp_ind = yp_ind\n",
    "    \n",
    "    yp_ar = []\n",
    "    for i in range(inf_iter):\n",
    "        gradients = compute_gradient(images, yp_ind)\n",
    "        \n",
    "        next_yp_ind = tf.math.add(current_yp_ind, tf.math.multiply(inf_rate, gradients))\n",
    "        \n",
    "        # Langevin dynamics\n",
    "        temp = tf.cast(inf_rate/2, tf.float32)\n",
    "        temp = temp * gradients + tf.random.normal(gradients.get_shape().as_list(), mean=0, stddev=inf_rate)\n",
    "        next_yp_ind = next_yp_ind + temp\n",
    "        current_yp_ind = next_yp_ind\n",
    "        \n",
    "        yp_ind = current_yp_ind\n",
    "        yp_ar.append(yp_ind)\n",
    "        \n",
    "    yp = yp_ar[-1]\n",
    "    yp = tf.nn.sigmoid(yp)\n",
    "    return yp\n",
    "\n",
    "def test_time_inf(images, fake_label, inf_iter, inf_rate):\n",
    "    yp_ind = fake_label;   current_yp_ind = yp_ind\n",
    "    \n",
    "    yp_ar = []\n",
    "    for i in range(inf_iter):\n",
    "        gradients = compute_gradient(images, yp_ind)\n",
    "        \n",
    "        next_yp_ind = tf.math.add(current_yp_ind, tf.math.multiply(inf_rate, gradients))\n",
    "        \n",
    "        # Langevin dynamics\n",
    "        temp = tf.cast(inf_rate/2, tf.float32)\n",
    "        temp = temp * gradients\n",
    "        next_yp_ind = next_yp_ind + temp\n",
    "        current_yp_ind = next_yp_ind\n",
    "        \n",
    "        yp_ind = current_yp_ind\n",
    "        yp_ar.append(yp_ind)\n",
    "        \n",
    "    yp = yp_ar[-1]\n",
    "    yp = tf.nn.sigmoid(yp)\n",
    "    return yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure(loss, arange, brange, start, end, step, plot_name):\n",
    "    axis_length = len(np.linspace(start, end, step))\n",
    "    loss = loss.numpy().reshape(-1, axis_length)\n",
    "\n",
    "    # plot the surface plot with plotly's Surface\n",
    "    fig = go.Figure(data=go.Surface(z=loss,\n",
    "                                    x=arange,\n",
    "                                    y=brange))\n",
    "\n",
    "    # add a countour plot\n",
    "    fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
    "                                      highlightcolor=\"limegreen\", project_z=True))\n",
    "\n",
    "    # annotate the plot\n",
    "    fig.update_layout(title=plot_name,\n",
    "                      scene=dict(\n",
    "                        xaxis_title='Pred. Label Axis-0',\n",
    "                        yaxis_title='Pred. Label Axis-1',\n",
    "                        zaxis_title=plot_name),\n",
    "                      width=700, height=700)\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "def generate_vectors(real_image, real_label, fake_label, start, end, step):\n",
    "    arange = np.linspace(start, end, step)\n",
    "    brange = np.linspace(start, end, step)\n",
    "\n",
    "    alen = arange.shape[0]\n",
    "    blen = brange.shape[0]\n",
    "\n",
    "    r1 = np.random.uniform(0, 1, (fake_label.shape[0], image_size * image_size))\n",
    "    r1[:,0] = 1 - r1[:,1]\n",
    "    r2 = np.random.uniform(0, 1, (fake_label.shape[0], image_size * image_size))\n",
    "    r2[:,0] = 1 - r2[:,1]\n",
    "    \n",
    "    y_img = np.zeros((alen*blen, fake_label.shape[0], image_size * image_size * 3))\n",
    "    y_pred = np.zeros((alen*blen, fake_label.shape[0], image_size * image_size))\n",
    "    y_true = np.zeros((alen*blen, fake_label.shape[0], image_size * image_size))\n",
    "    for b in range(0,blen):\n",
    "        for a in range(0,alen):\n",
    "            k = arange[a]\n",
    "            l = brange[b]\n",
    "            y_pred[b*alen+a] = tf.clip_by_value(fake_label[0, :] + l*r1[0, :] + k*r2[0, :], 0.0, 1.0) # just one horse\n",
    "            y_true[b*alen+a] = real_label[0, :]\n",
    "            y_img[b*alen+a] = real_image[0, :]\n",
    "    return arange, brange, y_img, y_pred, y_true\n",
    "\n",
    "def plot_contour_ce(real_image, real_label, inf_iter, inf_rate, start, end, step):\n",
    "    batch_size = real_label.shape[0]\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    fake_label = test_time_inf(real_image, z, inf_iter, inf_rate)\n",
    "    \n",
    "    fake_label = np.array(fake_label).reshape(-1, image_size * image_size)\n",
    "    real_label = np.array(real_label).reshape(-1, image_size * image_size)\n",
    "    real_image = np.array(real_image).reshape(-1, image_size * image_size * 3)\n",
    "\n",
    "    arange, brange, y_img, y_pred, y_true = generate_vectors(real_image, real_label, fake_label, start, end, step)\n",
    "\n",
    "    # CE Loss\n",
    "    loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    plot_figure(loss, arange, brange, start, end, step, 'Cross Entropy')\n",
    "    \n",
    "    \n",
    "def plot_contour_adv(real_image, real_label, inf_iter, inf_rate, start, end, step):\n",
    "    batch_size = real_label.shape[0]\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    fake_label = test_time_inf(real_image, z, inf_iter, inf_rate)\n",
    "    \n",
    "    fake_label = np.array(fake_label).reshape(-1, image_size * image_size)\n",
    "    real_label = np.array(real_label).reshape(-1, image_size * image_size)\n",
    "    real_image = np.array(real_image).reshape(-1, image_size * image_size * 3)\n",
    "    \n",
    "    # Adv loss\n",
    "    arange, brange, y_img, y_pred, y_true = generate_vectors(real_image, real_label, fake_label, start, end, step)\n",
    "    \n",
    "    real_image = y_img.reshape(-1, image_size, image_size, 3)\n",
    "    real_label = y_true.reshape(-1, image_size, image_size, 1)\n",
    "    fake_label = y_pred.reshape(-1, image_size, image_size, 1)\n",
    "    \n",
    "    # disc signals\n",
    "    real_score = dis(real_image, real_label, real_label)\n",
    "    fake_score = dis(real_image, real_label, fake_label)\n",
    "\n",
    "    # interpolate\n",
    "    alpha_ = tf.random.uniform([fake_label.shape[0], 1, 1, 1], 0.0, 1.0)\n",
    "    inter_sample = fake_label * alpha_ + real_label * (1 - alpha_)\n",
    "    with tf.GradientTape() as tape_gp:\n",
    "        tape_gp.watch(inter_sample)\n",
    "        inter_score = dis(real_image, real_label, inter_sample)\n",
    "    gp_gradients = tape_gp.gradient(inter_score, inter_sample)\n",
    "    \n",
    "    # gradient penalty\n",
    "    gp_gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gp_gradients), axis = [1, 2, 3]))\n",
    "    gp = tf.reduce_mean((gp_gradients_norm - 1.0) ** 2)\n",
    "\n",
    "    # loss\n",
    "    loss = fake_score - real_score + (gp * 10)\n",
    "    plot_figure(loss, arange, brange, start, end, step, 'Adversarial loss')\n",
    "    \n",
    "    \n",
    "def plot_hist(fake, real):\n",
    "    fake = np.array(fake).reshape(-1, 1)\n",
    "    real = np.array(real).reshape(-1, 1)\n",
    "    bins = len(fake)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.hist(fake, bins, alpha=0.5, color='blue', label='Fake')\n",
    "    plt.hist(real, bins, alpha=0.5, color='orange', label='Real')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def calc_iou(images, labels, inf_iter, inf_rate):\n",
    "    batch_size = labels.shape[0]\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    fake_label = test_time_inf(images, z, inf_iter, inf_rate)\n",
    "    return iou(labels, fake_label)\n",
    "\n",
    "def draw_test(images, labels, inf_iter, inf_rate, name):\n",
    "    batch_size = labels.shape[0]\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    fake_label = test_time_inf(images, z, inf_iter, inf_rate)\n",
    "    iou_score = iou(labels, fake_label)\n",
    "    print('\\n' + name + ' iou_score %:', round(iou_score*100, 2))\n",
    "    \n",
    "    for i in range(10):\n",
    "        draw_all(images[i], labels[i], fake_label[i])\n",
    "        \n",
    "def plot_iou(epoch, train_iou, val_iou, test_iou):\n",
    "    plt.plot(np.arange(epoch+1), np.array(train_iou), label = \"train\")\n",
    "    plt.plot(np.arange(epoch+1), np.array(val_iou), label = \"test\")\n",
    "    plt.plot(np.arange(epoch+1), np.array(test_iou), label = \"valid\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IOU Score')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_gen(real_image, real_label, inf_iter, inf_rate, pt_loss, alpha):\n",
    "    batch_size = real_image.get_shape().as_list()[0]\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    real_label = tf.cast(real_label, dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_label = unrolled_inf(real_image, z, inf_iter, inf_rate)\n",
    "        fake_score = dis(real_image, real_label, fake_label)\n",
    "        loss = tf.reduce_mean(fake_score) + (pt_loss * alpha)\n",
    "    gradients = tape.gradient(loss, gen.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(gradients, gen.trainable_variables))\n",
    "    gen_loss(loss)\n",
    "\n",
    "def train_step_dis(real_image, real_label, inf_iter, inf_rate):\n",
    "    batch_size = real_image.get_shape().as_list()[0]\n",
    "    real_label = tf.cast(real_label, dtype=tf.float32)\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_label = unrolled_inf(real_image, z, inf_iter, inf_rate)\n",
    "        real_score = dis(real_image, real_label, real_label)\n",
    "        fake_score = dis(real_image, real_label, fake_label)\n",
    "        \n",
    "        alpha_ = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        inter_sample = fake_label * alpha_ + real_label * (1 - alpha_)\n",
    "        with tf.GradientTape() as tape_gp:\n",
    "            tape_gp.watch(inter_sample)\n",
    "            inter_score = dis(real_image, real_label, inter_sample)\n",
    "        gp_gradients = tape_gp.gradient(inter_score, inter_sample)\n",
    "        \n",
    "        # gradient penalty\n",
    "        gp_gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gp_gradients), axis = [1, 2, 3]))\n",
    "        gp = tf.reduce_mean((gp_gradients_norm - 1.0) ** 2)\n",
    "        \n",
    "        # adv loss\n",
    "        loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score) + (gp * 10)\n",
    "    \n",
    "    gradients = tape.gradient(loss, dis.trainable_variables)\n",
    "    dis_opt.apply_gradients(zip(gradients, dis.trainable_variables))\n",
    "    \n",
    "    dis_loss(loss)\n",
    "    adv_loss(loss - gp * 10)\n",
    "    return fake_score, real_score\n",
    "    \n",
    "def pretrain_gen(real_image, real_label, inf_iter, inf_rate):\n",
    "    batch_size = real_image.get_shape().as_list()[0]\n",
    "    real_label = tf.cast(real_label, dtype=tf.float32)\n",
    "    z = tf.random.uniform([batch_size, image_size, image_size, 1], 0.0, 1.0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_label = unrolled_inf(real_image, z, inf_iter, inf_rate)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_label, fake_label))\n",
    "    gradients = tape.gradient(loss, gen.trainable_variables)\n",
    "    gen_opt_pt.apply_gradients(zip(gradients, gen.trainable_variables))\n",
    "    gen_pretrain_loss(loss)\n",
    "    return loss\n",
    "\n",
    "def train(data, n_epoch, n_update_dis, inf_iter, inf_rate, pt_gen, alpha):\n",
    "    train_iou = []; val_iou = []; test_iou = [];\n",
    "    for epoch in range(n_epoch):\n",
    "        print('\\nepoch:', epoch)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            pt_loss = 0\n",
    "        \n",
    "        fake_store = [];   real_store = [];\n",
    "        for images, labels in train_dataset:\n",
    "            \n",
    "            # train gen\n",
    "            if epoch > 0:\n",
    "                if pt_gen:\n",
    "                    pt_loss = pretrain_gen(images, labels, inf_iter, inf_rate)\n",
    "                train_step_gen(images, labels, inf_iter, inf_rate, pt_loss, alpha)\n",
    "            \n",
    "            # train disc\n",
    "            for i in range(n_update_dis):\n",
    "                fake_sc, real_sc = train_step_dis(images, labels, inf_iter, inf_rate)\n",
    "            fake_store.append(fake_sc.numpy())\n",
    "            real_store.append(real_sc.numpy())\n",
    "            \n",
    "        # store iou progression\n",
    "        train_iou.append(calc_iou(xdata, ydata, inf_iter, inf_rate))\n",
    "        val_iou.append(calc_iou(xtest, ytest, inf_iter, inf_rate))\n",
    "        test_iou.append(calc_iou(xval, yval, inf_iter, inf_rate))\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            plot_hist(fake_store, real_store)\n",
    "            \n",
    "            start, end, step = 0.0, 20.0, 10\n",
    "            plot_contour_ce(xtest, ytest, inf_iter, inf_rate, start, end, step)\n",
    "            \n",
    "            start, end, step = -200.0, 100.0, 10\n",
    "            plot_contour_adv(xtest, ytest, inf_iter, inf_rate, start, end, step)\n",
    "\n",
    "            draw_test(xdata, ydata, inf_iter, inf_rate, 'train')\n",
    "            draw_test(xtest, ytest, inf_iter, inf_rate, 'test')\n",
    "            draw_test(xval, yval, inf_iter, inf_rate, 'valid')\n",
    "            \n",
    "            plot_iou(epoch, train_iou, val_iou, test_iou)\n",
    "                \n",
    "        template = 'PT Gen Loss: {}, Gen Loss: {}, Dis Loss: {}, Adv Loss: {}'\n",
    "        print (template.format(gen_pretrain_loss.result(), gen_loss.result(), dis_loss.result(), adv_loss.result()))\n",
    "        dis_loss.reset_states()\n",
    "        adv_loss.reset_states()\n",
    "        gen_loss.reset_states()\n",
    "        gen_pretrain_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "batch_size = 8\n",
    "n_epoch = 300\n",
    "n_figs = 10\n",
    "\n",
    "# disc\n",
    "n_update_dis = 5\n",
    "d_learning_rate = 0.0005\n",
    "\n",
    "# pretrain gen\n",
    "alpha = 0.01\n",
    "pt_gen = True\n",
    "pt_g_learning_rate = 0.001\n",
    "\n",
    "# ebm gen\n",
    "inf_iter = 10 #alpha\n",
    "inf_rate = 2 #delta\n",
    "g_learning_rate = 0.0005\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "# create tf dataset generator object\n",
    "N = len(xdata)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((xdata, ydata))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=False)\n",
    "\n",
    "# Initialize Networks\n",
    "gen = Generator()\n",
    "dis = Discriminator()\n",
    "\n",
    "# Initialize Optimizer\n",
    "gen_opt = tf.keras.optimizers.Adam(g_learning_rate)\n",
    "gen_opt_pt = tf.keras.optimizers.Adam(pt_g_learning_rate)\n",
    "dis_opt = tf.keras.optimizers.Adam(d_learning_rate)\n",
    "\n",
    "# Initialize Metrics\n",
    "adv_loss = tf.keras.metrics.Mean(name = 'Adversarial_Loss')\n",
    "dis_loss = tf.keras.metrics.Mean(name = 'Discriminator_Loss')\n",
    "gen_loss = tf.keras.metrics.Mean(name = 'Generator_Loss')\n",
    "gen_pretrain_loss = tf.keras.metrics.Mean(name = 'Generator_Pretrain_Loss')\n",
    "\n",
    "train(train_dataset, n_epoch, n_update_dis, inf_iter, inf_rate, pt_gen, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
